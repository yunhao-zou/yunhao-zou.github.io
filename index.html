<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yunhao ZOU </title> <meta name="author" content="Yunhao ZOU"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?2b713c291d2402e9360ae0e1f5314f5a"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yunhao-zou.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%7A%6F%75%79%75%6E%68%61%6F@%62%69%74.%65%64%75.%63%6E" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=52s5QEQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Yunhao-Zou/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/yunhao-zou" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yunhao</span> ZOU </h1> <p class="desc">Ph.D. candidate, Beijing Institute of Technology</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic.jpg" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?fb10d6474404d4f0c19bc3d86d59c448" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I’m a final-year Ph.D. candidate at <a href="https://bit.edu.cn/" rel="external nofollow noopener" target="_blank">Beijing Institute of Technology</a>, China, supervised by <a href="https://ying-fu.github.io/" rel="external nofollow noopener" target="_blank">Prof. Ying Fu</a>. I’m also currently a visiting Ph.D. student at <a href="https://www.uni-wuerzburg.de/" rel="external nofollow noopener" target="_blank">the University of Würzburg</a>, Germany, working with <a href="https://www.informatik.uni-wuerzburg.de/computervision/" rel="external nofollow noopener" target="_blank">Prof. Radu Timofte</a>. Before doctoral study, I obtained my B.S. in Computer Science from <a href="https://bit.edu.cn/" rel="external nofollow noopener" target="_blank">Beijing Institute of Technology</a> in 2019.</p> <p>I research in various tasks in low-level vision, with extensive experience in computational photography, image denoising, HDR image/video reconstruction, and low-light enhancement. I also had research experiences in computational imaging and hyperspectral imaging. I am enthusiastic in enhancing the quality of everyday images and videos through AI technologies.</p> <p style="width: 80%; color: #b71c1c;">I am looking for positions in both academia and industry starting in Fall 2025. Feel free to connect with me if you are interested in working with me. </p> <p><a href="mailto:zouyunhao@bit.edu.cn">Email</a> <span style="visibility: hidden;">.</span> / <span style="visibility: hidden;">.</span> <a href="https://scholar.google.com/citations?user=52s5QEQAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a> <span style="visibility: hidden;">.</span> / <span style="visibility: hidden;">.</span> <a href="https://github.com/yunhao-zou" rel="external nofollow noopener" target="_blank">Github</a></p> </div> <h2 style="margin-top: 7px; margin-bottom: 5px;"> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 12, 2024</th> <td> One paper is accepted by TPAMI 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 19, 2024</th> <td> Our NightSightHD wins the gold award at the 49th Geneva International Exhibition of Inventions！ </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 27, 2024</th> <td> One paper is accepted by CVPR 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 22, 2023</th> <td> One paper is accepted by TGRS 2023! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 13, 2023</th> <td> One paper is accepted by TCSVT 2023! </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 14, 2023</th> <td> Two papers are accepted by ICCV 2023! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 03, 2022</th> <td> One paper is accepted by CVPR 2022! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 01, 2021</th> <td> One paper is accepted by CVPR 2021! </td> </tr> </table> </div> </div> <h2 style="margin-top: 7px; margin-bottom: 5px;"> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/event_pami.png" sizes="200px"> <img src="/assets/img/publication_preview/event_pami.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="event_pami.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zou2024learning" class="col-sm-8"> <div class="title">EventHDR: from Event to High-Speed HDR Videos and Beyond</div> <div class="author"> <b style="color: #b71c1c;">Yunhao Zou</b>, Ying Fu, Tsuyoshi Takatani, and Yinqiang Zheng </div> <div class="periodical"> <em>IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/yunhao-zou/EventHDR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Event cameras are innovative neuromorphic sensors that asynchronously capture the scene dynamics. Due to the eventtriggering mechanism, such cameras record event streams with much shorter response latency and higher intensity sensitivity compared to conventional cameras. On the basis of these features, previous works have attempted to reconstruct high dynamic range (HDR) videos from events, but have either suffered from unrealistic artifacts or failed to provide sufficiently high frame rates. In this paper, we present a recurrent convolutional neural network that reconstruct high-speed HDR videos from event sequences, with a key frame guidance to prevent potential error accumulation caused by the sparse event data. Additionally, to address the problem of severely limited real dataset, we develop a new optical system to collect a real-world dataset with paired high-speed HDR videos and event streams, facilitating future research in this field. Our dataset provides the first real paired dataset for event-to-HDR reconstruction, avoiding potential inaccuracies from simulation strategies. Experimental results demonstrate that our method can generate high-quality, highspeed HDR videos. We further explore the potential of our work in cross-camera reconstruction and downstream computer vision tasks, including object detection, panoramic segmentation, optical flow estimation, and monocular depth estimation under HDR scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zou2024learning</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zou, Yunhao and Fu, Ying and Takatani, Tsuyoshi and Zheng, Yinqiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EventHDR: from Event to High-Speed HDR Videos and Beyond}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-18}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rawhdr.jpg" sizes="200px"> <img src="/assets/img/publication_preview/rawhdr.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rawhdr.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zou2023rawhdr" class="col-sm-8"> <div class="title">RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image</div> <div class="author"> <b style="color: #b71c1c;">Yunhao Zou</b>, Chenggang Yan, and Ying Fu </div> <div class="periodical"> <em>In Proc. of International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2309.02020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_RawHDR_High_Dynamic_Range_Image_Reconstruction_from_a_Single_Raw_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yunhao-zou/RawHDR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>High dynamic range (HDR) images capture much more intensity levels than standard ones. Current methods predominantly generate HDR images from 8-bit low dynamic range (LDR) sRGB images that have been degraded by the camera processing pipeline. However, it becomes a formidable task to retrieve extremely high dynamic range scenes from such limited bit-depth data. Unlike existing methods, the core idea of this work is to incorporate more informative Raw sensor data to generate HDR images, aiming to recover scene information in hard regions (the darkest and brightest areas of an HDR scene). To this end, we propose a model tailor-made for Raw images, harnessing the unique features of Raw data to facilitate the Raw-to-HDR mapping. Specifically, we learn exposure masks to separate the hard and easy regions of a high dynamic scene. Then, we introduce two important guidances, dual intensity guidance, which guides less informative channels with more informative ones, and global spatial guidance, which extrapolates scene specifics over an extended spatial domain. To verify our Raw-to-HDR approach, we collect a large Raw/HDR paired dataset for both training and testing. Our empirical evaluations validate the superiority of the proposed Raw-to-HDR reconstruction model, as well as our newly captured dataset in the experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zou2023rawhdr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zou, Yunhao and Yan, Chenggang and Fu, Ying}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proc. of International Conference on Computer Vision (ICCV)}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12334--12344}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iterative.png" sizes="200px"> <img src="/assets/img/publication_preview/iterative.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iterative.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zou2023iterative" class="col-sm-8"> <div class="title">Iterative Denoiser and Noise Estimator for Self-Supervised Image Denoising</div> <div class="author"> <b style="color: #b71c1c;">Yunhao Zou</b>, Chenggang Yan, and Ying Fu </div> <div class="periodical"> <em>In Proc. of International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Iterative_Denoiser_and_Noise_Estimator_for_Self-Supervised_Image_Denoising_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yunhao-zou/DCD-Net" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>With the emergence of powerful deep learning tools, more and more effective deep denoisers have advanced the f ield of image denoising. However, the huge progress made by these learning-based methods severely relies on largescale and high-quality noisy/clean training pairs, which limits the practicality in real-world scenarios. To overcome this, researchers have been exploring self-supervised approaches that can denoise without paired data. However, the unavailable noise prior and inefficient feature extraction take these methods away from high practicality and precision. In this paper, we propose a Denoise-CorruptDenoise pipeline (DCD-Net) for self-supervised image denoising. Specifically, we design an iterative training strategy, which iteratively optimizes the denoiser and noise estimator, and gradually approaches high denoising performances using only single noisy images without any noise prior. The proposed self-supervised image denoising framework provides very competitive results compared with stateof-the-art methods on widely used synthetic and real-world image denoising benchmarks</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zou2023iterative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Iterative Denoiser and Noise Estimator for Self-Supervised Image Denoising}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zou, Yunhao and Yan, Chenggang and Fu, Ying}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proc. of International Conference on Computer Vision (ICCV)}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13265--13274}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/noise_estimate.png" sizes="200px"> <img src="/assets/img/publication_preview/noise_estimate.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="noise_estimate.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zou2022estimating" class="col-sm-8"> <div class="title">Estimating Fine-Grained Noise Model via Contrastive Learning</div> <div class="author"> <b style="color: #b71c1c;">Yunhao Zou</b>, and Ying Fu </div> <div class="periodical"> <em>In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2204.01716" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zou_Estimating_Fine-Grained_Noise_Model_via_Contrastive_Learning_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yunhao-zou/Deep-Noise-Estimation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Image denoising has achieved unprecedented progress as great efforts have been made to exploit effective deep denoisers. To improve the denoising performance in realworld, two typical solutions are used in recent trends: devising better noise models for the synthesis of more realistic training data, and estimating noise level function to guide non-blind denoisers. In this work, we combine both noise modeling and estimation, and propose an innovative noise model estimation and noise synthesis pipeline for realistic noisy image generation. Specifically, our model learns a noise estimation model with fine-grained statistical noise model in a contrastive manner. Then, we use the estimated noise parameters to model camera-specific noise distribution, and synthesize realistic noisy training data. The most striking thing for our work is that by calibrating noise models of several sensors, our model can be extended to predict other cameras. In other words, we can estimate cameraspecific noise models for unknown sensors with only testing images, without laborious calibration frames or paired noisy/clean data. The proposed pipeline endows deep denoisers with competitive performances with state-of-the-art real noise modeling methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zou2022estimating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Estimating Fine-Grained Noise Model via Contrastive Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zou, Yunhao and Fu, Ying}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proc. of Conference on Computer Vision and Pattern Recognition (CVPR)}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12682--12691}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/event_cvpr.jpg" sizes="200px"> <img src="/assets/img/publication_preview/event_cvpr.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="event_cvpr.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zou2021learning" class="col-sm-8"> <div class="title">Learning to Reconstruct High Speed and High Dynamic Range Videos from Events</div> <div class="author"> <b style="color: #b71c1c;">Yunhao Zou</b>, Yinqiang Zheng, Tsuyoshi Takatani, and Ying Fu </div> <div class="periodical"> <em>In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zou_Learning_To_Reconstruct_High_Speed_and_High_Dynamic_Range_Videos_CVPR_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yunhao-zou/EventHDR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Event cameras are novel sensors that capture the dynamics of a scene asynchronously. Such cameras record event streams with much shorter response latency than images captured by conventional cameras, and are also highly sensitive to intensity change, which is brought by the triggering mechanism of events. On the basis of these two features, previous works attempt to reconstruct high speed and high dynamic range (HDR) videos from events. However, these works either suffer from unrealistic artifacts, or cannot provide sufficiently high frame rate. In this paper, we present a convolutional recurrent neural network which takes a sequence of neighboring events to reconstruct high speed HDRvideos, and temporal consistency is well considered to facilitate the training process. In addition, we setup a prototype optical system to collect a real-world dataset with paired high speed HDR videos and event streams, which will be made publicly accessible for future researches in this field. Experimental results on both simulated and real scenes verify that our method can generate high speed HDR videos with high quality, and outperform the state-of-the-art reconstruction methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zou2021learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Reconstruct High Speed and High Dynamic Range Videos from Events}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zou, Yunhao and Zheng, Yinqiang and Takatani, Tsuyoshi and Fu, Ying}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proc. of Conference on Computer Vision and Pattern Recognition (CVPR)}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2024--2033}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <h2 style="margin-top: 10px; margin-bottom: 10px;"> Hornors and Awards </h2> <ul style="padding-left: 1.2em;"> <li>Gold Award of the 49th Geneva International Invention Exhibition, 2024</li> <li>Runner-up award of CVPR PBDL Highspeed HDR Video Reconstruction from Events challenge, 2024</li> <li>Toyou Technologies Scholarship, Toyou Feiji Electronics Co., Ltd., 2022</li> <li>China National Scholarship, Ministry of Education (Top 2% in the Ph.D Track), 2021</li> <li>Excellent Student Award, Beijing Institute of Technology, 2020</li> </ul> <h2 style="margin-top: 10px; margin-bottom: 10px;"> Services </h2> <ul style="padding-left: 1.2em;"> <li>Reviewer for Conferences: CVPR, ICCV, ECCV, AAAI, BMVC, ACCV, ICPR, CICAI</li> <li>Reviewer for Journals: TPAMI, Neurocomputing, Displays, IJRS</li> <li>Co-proposer and presenter for CVPR 2023 tutorial: <a href="https://cvpr.thecvf.com/virtual/2023/tutorial/18563" rel="external nofollow noopener" target="_blank"><b>Optics for Better AI: Capturing and Synthesizing Realistic Data for Low-light Enhancement</b></a> </li> </ul> <div class="social"> <div class="contact-icons"> <a href="mailto:%7A%6F%75%79%75%6E%68%61%6F@%62%69%74.%65%64%75.%63%6E" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=52s5QEQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Yunhao-Zou/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/yunhao-zou" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yunhao ZOU. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated on Sep. 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>